# WU-THEO-103: Survey of Calibration in Production Systems

**Work Unit**: WU-THEO-103
**Type**: Theoretical Research
**Date**: 2026-01-27
**Status**: Complete

## Executive Summary

This survey examines industry practices for confidence calibration in production AI systems. Calibration ensures that predicted probabilities accurately reflect true likelihoods, enabling reliable decision-making. The research covers production ML systems at major tech companies, LLM-specific practices, calibration infrastructure, domain-specific requirements, and available tools.

**Key Findings**:
1. Production systems predominantly use Platt scaling (sigmoid) and isotonic regression, with temperature scaling for neural networks
2. LLM calibration remains an open challenge - token probabilities do not equal semantic confidence
3. Calibration drift monitoring is essential; hybrid trigger-based and scheduled recalibration is the industry norm
4. Domain-specific requirements vary significantly (FDA mandates for medical AI vs. speed requirements in finance)
5. Multiple mature tools exist but lack integrated LLM-specific calibration support

---

## 1. Production ML Systems Calibration

### 1.1 Major Company Practices

#### Netflix ML Platform
Netflix's Machine Learning Platform (MLP) team uses [Metaflow](https://metaflow.org/), an open-source ML infrastructure framework, to support hundreds of ML projects across content demand modeling, media processing, and recommendation systems. Key design principles include minimizing:
- The "House of Cards effect" (shaky underlying layers)
- The "Puzzle effect" (unintuitive interfaces)
- The "Waterbed effect" (complexity that resurfaces elsewhere)

Recent Metaflow features (2024-2025) include checkpointing for long-running model training and new configuration objects, though calibration-specific features are not explicitly documented.

#### General Industry Approach
Major tech companies typically implement calibration as a post-processing step in their ML pipelines. The specific calibration method depends on:
- Model type (SVMs, boosted trees, neural networks)
- Dataset size (isotonic requires >1000 samples)
- Prediction speed requirements
- Interpretability needs

### 1.2 Calibration Methods at Scale

#### Platt Scaling (Sigmoid Calibration)
- Fits a logistic regression model to classifier scores
- Particularly effective for SVMs and boosted trees
- Assumes sigmoidal distortion in predicted probabilities
- Works well with limited calibration data
- Fast inference (single sigmoid transform)

**Production characteristics**:
- Low computational overhead
- Parametric (2 parameters only)
- May underfit complex calibration curves

#### Isotonic Regression
- Non-parametric, step-wise monotonic calibration
- More flexible than Platt scaling
- Requires large calibration sets (>1000 samples recommended)
- Can overfit on small datasets

**Production characteristics**:
- Higher memory footprint (stores calibration points)
- Better for complex calibration curves
- Requires more calibration data

#### Temperature Scaling
- Divides logits by learned scalar parameter T
- Single-parameter post-processing
- Near-perfect calibration restoration for neural networks
- Millisecond execution time
- Based on [Guo et al., "On Calibration of Modern Neural Networks"](https://geoffpleiss.com/blog/nn_calibration.html)

**Production characteristics**:
- Minimal overhead (single division)
- Classification only
- T > 1 indicates model overconfidence

### 1.3 Post-Deployment Calibration Drift

#### Types of Drift
1. **Data Drift**: Changes in feature distributions
2. **Concept Drift**: Changes in relationships between features and targets
3. **Training-Serving Skew**: Differences between training and production data

#### Detection Methods
- Statistical tests (Kolmogorov-Smirnov, chi-squared)
- Distance metrics (Wasserstein, PSI, K-L divergence)
- Model performance degradation monitoring
- Calibration curve comparison over time

#### Recalibration Strategies
Production systems use two main approaches:

**Schedule-Based**:
- Daily: High-frequency trading, real-time bidding
- Weekly: Most production systems (sweet spot)
- Monthly/Quarterly: Stable domains

**Trigger-Based**:
- Performance below threshold
- Significant data drift detected
- External events (regulatory changes, market shifts)

**Best Practice**: Hybrid approach combining predictable scheduling with responsiveness to genuine drift.

---

## 2. LLM-Specific Calibration Practices

### 2.1 AI API Provider Approaches

#### OpenAI
- Exposes logprobs via API (`logprobs: true`, `top_logprobs: 0-5`)
- Returns log probabilities for each output token
- Includes top alternative tokens at each position
- Enables confidence-based routing (small model -> large model when uncertain)

#### Anthropic
- Does **not** expose token logprobs
- Techniques requiring logprobs cannot be used with Claude models
- Forces reliance on verbalized confidence or sampling-based methods

#### Other Providers
- Mistral, Together.ai, Groq: Generally support logprobs
- [LiteLLM](https://github.com/BerriAI/litellm): Unified interface for 100+ APIs with varying logprob support

### 2.2 Token-Level vs Response-Level Confidence

#### Token-Level Methods
- **Token-Level Probability (TLP)**: Analyze probability distribution over generated tokens
- **Token-Level Entropy**: High entropy = uncertain, low entropy = confident
- Limitation: Token probabilities != semantic correctness

**Research finding**: Variable names have low token probability but are not errors; weather statements have high probability but may lack factual grounding.

#### Response-Level Methods
- **Sample Consistency (SC)**: Generate multiple responses, measure agreement
- SC by sentence embedding: ROC AUC 0.68-0.79 but poor calibration
- SC by GPT annotation: ROC AUC 0.66-0.74 with accurate calibration
- **Verbalized Confidence**: Ask model to state confidence (e.g., "85% confident")

#### Key Challenges
1. **Overconfidence**: LLMs consistently overestimate confidence (80-100% range, multiples of 5)
2. **Token vs Semantic Gap**: Next-token calibration doesn't capture response-level uncertainty
3. **Long-form Answers**: Multiple valid phrasings complicate probability interpretation

### 2.3 Uncertainty Quantification Categories

Per [ACM Computing Surveys](https://dl.acm.org/doi/10.1145/3744238):

| Method | Pros | Cons |
|--------|------|------|
| Logit-based | Fast, internal | Not available for all APIs |
| Sampling-based | Semantic insight | Computationally expensive |
| Verbalized | Human-readable | Overconfident by default |

**Research finding**: White-box methods outperform black-box, but gap is narrow (0.522 vs 0.605 AUROC).

---

## 3. Calibration Infrastructure

### 3.1 Online vs Offline Calibration

#### Offline (Static) Calibration
- Train calibrator once on validation set
- Deploy with model
- Verify before production
- Less monitoring overhead
- Risk: Calibration staleness

#### Online (Dynamic) Calibration
- Continuously update calibrator
- Adapts to distribution shifts
- Requires robust monitoring
- Risk: Bad data corrupts calibrator

#### Hybrid Approach (Recommended)
- Initial offline calibration
- Periodic recalibration (weekly/monthly)
- Trigger-based emergency recalibration
- Shadow mode testing before deployment

### 3.2 A/B Testing for Calibration

Per [Apple's Bayesian A/B Testing research](https://machinelearning.apple.com/research/rapid-scalable):

**Challenges**:
- Lack of statistical power in multivariate designs
- Correlations between factors
- Need for sequential testing for early stopping
- Inability to pool knowledge from past tests

**Solutions**:
- Hierarchical Bayesian estimation
- Exploit correlations between factors
- Progressive learning from past experiments

**Best Practices**:
- Champion/challenger model comparison
- Prevent users from switching between control/experiment groups
- Measure real-world effectiveness, not just offline metrics

### 3.3 Monitoring Calibration Drift

#### Tools
- **[Evidently AI](https://www.evidentlyai.com/)**: 20+ statistical tests, drift detection, calibration monitoring
- **[Arize AI Phoenix](https://phoenix.arize.com/)**: Reliability diagrams, calibration curves, real-time monitoring
- **Azure ML Model Monitoring**: Built-in drift detection for online endpoints
- **SageMaker Model Monitor**: Automated drift detection with recalibration triggers

#### Metrics to Monitor
- Expected Calibration Error (ECE)
- Maximum Calibration Error (MCE)
- Brier Score
- Log Loss
- Reliability diagram shape

### 3.4 Recalibration Triggers

Per [industry research](https://smartdev.com/ai-model-drift-retraining-a-guide-for-ml-system-maintenance/):

| Trigger Type | Example | Use Case |
|--------------|---------|----------|
| Performance threshold | F1 < 0.70 | Classification tasks |
| Data drift detection | PSI > 0.25 | Feature distribution shifts |
| Scheduled | Weekly at 2 AM | Predictable updates |
| External event | Regulatory change | Compliance requirements |

**Implementation Pattern**:
```
EventBridge schedule → SageMaker pipeline → Recalibrate → Shadow test → Deploy
```

---

## 4. Domain-Specific Approaches

### 4.1 Medical AI Calibration

#### Regulatory Requirements (FDA)
Per [FDA AI/ML guidance](https://www.fda.gov/medical-devices/medical-device-regulatory-science-research-programs-conducted-osel/evaluation-methods-artificial-intelligence-ai-enabled-medical-devices-performance-assessment-and):

- **GMLP Principles**: 10 guiding principles for AI/ML medical devices
- **Uncertainty Quantification**: FDA investigates methods for evaluating classification, regression, and risk assessment models
- **PCCPs**: Predetermined Change Control Plans allow pre-approved modifications
- **Continuous Monitoring**: Required for deployed AI medical devices
- **1,250+ AI-enabled devices** authorized as of July 2025

**Key Requirements**:
1. Calibrated uncertainty outputs for clinician decision-making
2. Testing under clinically relevant conditions
3. Continuous monitoring post-deployment
4. Mechanisms for managing re-training risks

**Clinical Decision Support Exemptions**:
CDS software may avoid FDA regulation if it:
- Doesn't acquire/process medical images
- Presents data healthcare professionals typically exchange
- Provides recommendations (not specific actions)
- Allows independent review

#### Calibration Findings in Medical AI
Per [PMC research](https://pmc.ncbi.nlm.nih.gov/articles/PMC11648734/):
- Token probabilities outperform verbalized confidence (AUROC 0.71-0.87)
- All models demonstrate overconfidence
- Sample consistency methods achieve best discrimination

### 4.2 Financial AI Calibration

#### Characteristics
- Speed-critical: Millisecond latency requirements
- High stakes: Erroneous trades have immediate impact
- Regulatory scrutiny: ISDA and other body requirements

#### Approaches
Per [industry research](https://link.springer.com/article/10.1007/s11147-021-09183-7):

- **Deep calibration**: Neural networks for model parameter estimation
- **ANN-based frameworks**: 4x faster, more stable parameters
- **Parallel running**: New models run alongside existing systems
- **Stress testing**: Rigorous before deployment

**Best Practices**:
1. Run AI models in parallel with existing systems initially
2. Implement strict guardrails
3. Have controls and fallbacks for failures
4. Focus on enhancing traditional models, not replacing them

### 4.3 Code Generation Calibration

#### Research Findings
Per [ACM TOCHI research](https://dl.acm.org/doi/10.1145/3702320):

- Generation probabilities =/= error likelihood
- Variable naming has low probability but isn't an error
- Surrogate models better calibrated than linguistic outputs

#### Effective Approaches
- **Highlight predicted edits**: Not low-probability tokens
- **Multicalibration**: +0.37 skill score improvement
- **Localized uncertainty**: Predict which parts need attention

**User Study Results**:
- Highlighting predicted edit locations → faster task completion
- Highlighting low-probability tokens → no benefit over baseline

---

## 5. Tools and Frameworks

### 5.1 scikit-learn Calibration

**[CalibratedClassifierCV](https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html)**

```python
from sklearn.calibration import CalibratedClassifierCV

# Two approaches:
# 1. Prefit: Calibrate already-trained model
calibrated = CalibratedClassifierCV(clf, method='sigmoid', cv='prefit')
calibrated.fit(X_val, y_val)

# 2. Cross-validation: Train + calibrate with CV
calibrated = CalibratedClassifierCV(clf, method='isotonic', cv=5)
calibrated.fit(X_train, y_train)
```

**Methods**:
- `method='sigmoid'`: Platt scaling (default)
- `method='isotonic'`: Isotonic regression

**Best Practices**:
- Use isotonic only with >1000 calibration samples
- Keep calibration data separate from training data
- LogisticRegression often already well-calibrated

### 5.2 PyTorch Temperature Scaling

**[temperature_scaling](https://github.com/gpleiss/temperature_scaling)**

```python
from temperature_scaling import ModelWithTemperature

# Calibrate existing model
calibrated_model = ModelWithTemperature(model)
calibrated_model.set_temperature(valid_loader)

# Temperature > 1: Model was overconfident
# Temperature < 1: Model was underconfident
```

**Results**:
- ECE reduction: 2.10% → 0.25%
- MCE reduction: 27.27% → 3.86%

### 5.3 TensorFlow Temperature Scaling

```python
# Learn temperature parameter
temp_var = tf.Variable(1.0, name='temperature')
scaled_logits = logits / temp_var

# Optimize with cross-entropy loss
loss = tf.nn.softmax_cross_entropy_with_logits(labels, scaled_logits)
optimizer.minimize(loss, var_list=[temp_var])
```

### 5.4 ML Experiment Tracking

#### MLflow
- Open-source, self-hosted
- Auto-logging for TensorFlow, PyTorch, scikit-learn
- Model registry and versioning
- Requires infrastructure maintenance

#### Weights & Biases
- Cloud-based, managed
- Real-time metric synchronization
- Automatic experiment reproduction
- GPU utilization tracking
- Better for deep learning workflows

#### Recommendation
- **MLflow**: Structured ML workflows, on-premise requirements
- **W&B**: Dynamic deep learning, cloud-first teams

### 5.5 Production Monitoring Tools

| Tool | Type | Calibration Features |
|------|------|---------------------|
| [Evidently AI](https://github.com/evidentlyai/evidently) | Open-source | Reliability diagrams, drift detection, 20+ metrics |
| [Arize Phoenix](https://github.com/Arize-ai/phoenix) | Open-source | Calibration curves, LLM observability |
| Azure ML Monitor | Cloud | Built-in drift detection, auto-alerts |
| SageMaker Monitor | Cloud | Automated recalibration triggers |
| Datadog ML | Cloud | Model monitoring, alerting |

### 5.6 Custom Calibration Pipeline Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                    Production Calibration Pipeline               │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐  │
│  │  Model   │───▶│ Calibra- │───▶│ Shadow   │───▶│ Deploy   │  │
│  │ Training │    │   tor    │    │  Test    │    │          │  │
│  └──────────┘    └──────────┘    └──────────┘    └──────────┘  │
│                        ▲                              │          │
│                        │                              │          │
│                        │         ┌──────────┐         │          │
│                        └─────────│ Monitor  │◀────────┘          │
│                                  │ Service  │                    │
│                                  └──────────┘                    │
│                                       │                          │
│                                       ▼                          │
│                              ┌──────────────┐                    │
│                              │  Recalibrate │                    │
│                              │   Triggers   │                    │
│                              └──────────────┘                    │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## 6. Recommendations for Librarian

### 6.1 Calibration Strategy

Given Librarian's focus on code understanding with confidence scoring, the following approach is recommended:

#### Short-Term (Immediate Implementation)
1. **Use temperature scaling** for any neural network components
2. **Implement Platt scaling** for non-neural classifiers
3. **Track calibration metrics** (ECE, Brier score) from day one
4. **Log confidence scores** alongside predictions for analysis

#### Medium-Term (Infrastructure)
1. **Build reliability diagrams** into evaluation pipeline
2. **Implement drift detection** using Evidently AI or similar
3. **Create recalibration triggers** based on:
   - Weekly schedule (baseline)
   - ECE exceeding threshold (0.05 recommended)
   - Data drift detection (PSI > 0.25)
4. **A/B test calibration methods** on evaluation corpus

#### Long-Term (LLM-Specific)
1. **Don't rely solely on logprobs** - they don't equal semantic correctness
2. **Implement sampling-based consistency** for high-stakes predictions
3. **Build surrogate calibration models** trained on prediction correctness
4. **Consider multicalibration** approaches for code generation confidence

### 6.2 Code Generation Specific Recommendations

Based on research findings:

1. **Highlight predicted edit locations** rather than low-probability tokens
2. **Train edit prediction models** to estimate which parts users will modify
3. **Localized confidence** is more valuable than global confidence
4. **Calibrate to correctness** not generation probability

### 6.3 Architecture Recommendations

```
┌─────────────────────────────────────────────────────────────────┐
│                 Librarian Calibration Architecture               │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Raw Prediction ──▶ Calibration Layer ──▶ Calibrated Confidence │
│                            │                                     │
│                     ┌──────┴──────┐                              │
│                     │             │                              │
│              Temperature    Isotonic/Platt                       │
│               Scaling        Scaling                             │
│            (NN outputs)   (classifiers)                          │
│                                                                  │
│  Monitoring:                                                     │
│  - Reliability diagrams (daily)                                  │
│  - ECE/MCE tracking (per batch)                                  │
│  - Drift detection (weekly)                                      │
│                                                                  │
│  Recalibration Triggers:                                         │
│  - ECE > 0.05                                                    │
│  - PSI > 0.25 on input features                                  │
│  - Weekly scheduled refresh                                      │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### 6.4 Tools to Integrate

| Tool | Purpose | Priority |
|------|---------|----------|
| scikit-learn CalibratedClassifierCV | Basic calibration | High |
| temperature_scaling (PyTorch) | Neural network calibration | High |
| Evidently AI | Drift monitoring, calibration tracking | Medium |
| Arize Phoenix | LLM observability, reliability diagrams | Medium |
| MLflow/W&B | Experiment tracking, metric logging | Medium |

### 6.5 Metrics to Track

**Primary**:
- Expected Calibration Error (ECE) - target < 0.05
- Brier Score
- Reliability diagram shape

**Secondary**:
- Maximum Calibration Error (MCE)
- Log Loss
- AUROC of confidence as correctness predictor

**Operational**:
- Calibration drift over time
- Distribution of confidence scores
- Correlation between confidence and user acceptance

---

## 7. Sources

### Production ML Systems
- [Netflix Metaflow](https://metaflow.org/)
- [Netflix Tech Blog - Supporting Diverse ML Systems](https://netflixtechblog.com/supporting-diverse-ml-systems-at-netflix-2d2e6b6d205d)
- [InfoQ - Netflix Metaflow at Scale](https://www.infoq.com/news/2024/03/netflix-metaflow/)

### Calibration Methods
- [scikit-learn Probability Calibration](https://scikit-learn.org/stable/modules/calibration.html)
- [Platt Scaling - Wikipedia](https://en.wikipedia.org/wiki/Platt_scaling)
- [Train in Data - Platt Scaling Guide](https://www.blog.trainindata.com/complete-guide-to-platt-scaling/)
- [Temperature Scaling - GitHub](https://github.com/gpleiss/temperature_scaling)
- [Neural Network Calibration Blog](https://geoffpleiss.com/blog/nn_calibration.html)

### LLM Calibration
- [Vellum - Understanding Logprobs](https://www.vellum.ai/blog/what-are-logprobs-and-how-can-you-use-them)
- [ACM Computing Surveys - LLM Uncertainty Survey](https://dl.acm.org/doi/10.1145/3744238)
- [ArXiv - Can LLMs Express Uncertainty](https://arxiv.org/abs/2306.13063)
- [ArXiv - Uncertainty Quantification Survey](https://arxiv.org/html/2503.15850v1)

### Drift and Monitoring
- [Evidently AI - Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift)
- [Evidently AI - Model Monitoring](https://www.evidentlyai.com/ml-in-production/model-monitoring)
- [Arize AI - Calibration Curves](https://arize.com/blog-course/what-is-calibration-reliability-curve/)
- [Datadog - ML Model Monitoring](https://www.datadoghq.com/blog/ml-model-monitoring-in-production-best-practices/)

### Domain-Specific
- [FDA - AI Medical Device Evaluation](https://www.fda.gov/medical-devices/medical-device-regulatory-science-research-programs-conducted-osel/evaluation-methods-artificial-intelligence-ai-enabled-medical-devices-performance-assessment-and)
- [PMC - Medical AI Calibration](https://pmc.ncbi.nlm.nih.gov/articles/PMC11648734/)
- [Springer - Deep Calibration in Finance](https://link.springer.com/article/10.1007/s11147-021-09183-7)
- [ACM TOCHI - Code Generation Uncertainty](https://dl.acm.org/doi/10.1145/3702320)

### A/B Testing and Infrastructure
- [Apple ML Research - Bayesian A/B Testing](https://machinelearning.apple.com/research/rapid-scalable)
- [ML in Production - A/B Testing Models](https://mlinproduction.com/ab-test-ml-models-deployment-series-08/)
- [Azure AI Foundry - A/B Experiments](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/a-b-experimentation)

### Retraining and Recalibration
- [SmartDev - AI Model Drift Guide](https://smartdev.com/ai-model-drift-retraining-a-guide-for-ml-system-maintenance/)
- [Neptune AI - Retraining Models](https://neptune.ai/blog/retraining-model-during-deployment-continuous-training-continuous-testing)
- [AWS - MLOps with SageMaker](https://aws.amazon.com/blogs/machine-learning/mlops-for-batch-inference-with-model-monitoring-and-retraining-using-amazon-sagemaker-hashicorp-terraform-and-gitlab-ci-cd/)

---

## 8. Conclusion

Calibration in production AI systems is a mature field for traditional ML but remains an active research area for LLMs. Key takeaways:

1. **Traditional ML calibration is solved**: Temperature scaling, Platt scaling, and isotonic regression work well with proper monitoring
2. **LLM calibration is challenging**: Token probabilities don't translate to semantic confidence; verbalized confidence is overconfident
3. **Monitoring is essential**: Calibration degrades over time; automated drift detection and recalibration are required
4. **Domain requirements vary**: Medical AI requires FDA compliance; finance requires speed; code generation benefits from localized uncertainty
5. **Tools exist**: scikit-learn, PyTorch temperature scaling, Evidently AI, and Arize Phoenix provide solid foundations

For Librarian, the recommended approach is to start with established calibration methods (temperature scaling for neural components), implement comprehensive monitoring from day one, and develop LLM-specific calibration strategies based on sampling consistency and surrogate models rather than raw logprobs.
